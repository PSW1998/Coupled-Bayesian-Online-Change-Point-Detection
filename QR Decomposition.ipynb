{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da43426d-0174-48a3-8d7b-d6814cf52335",
   "metadata": {},
   "source": [
    "# QR Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aab721-3bce-48a8-aee1-e6f083c8c797",
   "metadata": {},
   "source": [
    "Suppose we have $A \\in \\mathbb{R}^{m \\times n}$ representing a set of linearly\n",
    "independent basis vectors (so $m \\ge n$), and we want to find a series of\n",
    "orthonormal vectors $q_1, q_2, \\dots$ that span the successive subspaces\n",
    "$\\operatorname{span}(a_1)$, $\\operatorname{span}(a_1, a_2)$, etc. In other words,\n",
    "we want to find vectors $q_j$ and coefficients $r_{ij}$ such that\n",
    "$$\n",
    "\\left(\n",
    "\\begin{array}{cccc}\n",
    "| & | &        & | \\\\\n",
    "a_1 & a_2 & \\cdots & a_n \\\\\n",
    "| & | &        & |\n",
    "\\end{array}\n",
    "\\right)\n",
    "=\n",
    "\\left(\n",
    "\\begin{array}{cccc}\n",
    "| & | &        & | \\\\\n",
    "q_1 & q_2 & \\cdots & q_n \\\\\n",
    "| & | &        & |\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\left(\n",
    "\\begin{array}{cccc}\n",
    "r_{11} & r_{12} & \\cdots & r_{1n} \\\\\n",
    "0      & r_{22} & \\cdots & r_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0      & 0      & \\cdots & r_{nn}\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "We can write this column-wise as\n",
    "$$\n",
    "a_1 = r_{11} q_1 \n",
    "$$\n",
    "$$\n",
    "a_2 = r_{12} q_1 + r_{22} q_2\n",
    "$$\n",
    "$$\n",
    "\\vdots\n",
    "$$\n",
    "$$\n",
    "a_n = r_{1n} q_1 + \\cdots + r_{nn} q_n \n",
    "$$\n",
    "so we see $q_1$ spans the space of $a_1$, and $q_1$ and $q_2$ span the space\n",
    "of $\\{a_1, a_2\\}$, etc.\n",
    "\n",
    "In matrix notation, we have\n",
    "$$\n",
    "A = \\hat{Q}\\,\\hat{R}\n",
    "$$\n",
    "where $\\hat{Q}$ is $m \\times n$ with orthonormal columns and $\\hat{R}$ is\n",
    "$n \\times n$ and upper triangular. This is called a \\emph{reduced QR} or\n",
    "\\emph{economy-sized QR} factorization of $A$.\n",
    "\n",
    "A \\emph{full} QR factorization appends an additional $m-n$ orthonormal columns\n",
    "to $\\hat{Q}$ so it becomes a square, orthogonal matrix $Q$, which satisfies\n",
    "$QQ^\\top = Q^\\top Q = I$. Also, we append rows of zeros to $\\hat{R}$ so it\n",
    "becomes an $m \\times n$ matrix that is still upper triangular, called $R$. The zero entries in $R$ ``kill off'' the new columns in $Q$,\n",
    "so the result is the same as $\\hat{Q}\\,\\hat{R}$.\n",
    "\n",
    "QR decomposition is commonly used to solve systems of linear equations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7098dd-53ce-4d17-9d51-02a791d28094",
   "metadata": {},
   "source": [
    "# Gram-Schmidt Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95932784-53cb-4dde-8f76-27a571458ce8",
   "metadata": {},
   "source": [
    "Algorithm: QR decomposition via Gram–Schmidt\n",
    "\n",
    "Input:\n",
    "    A ∈ R^{m×n}\n",
    "\n",
    "Output:\n",
    "    Q ∈ R^{m×n}  with orthonormal columns\n",
    "    R ∈ R^{n×n}  upper triangular\n",
    "    such that A = Q R\n",
    "\n",
    "1. Let (m, n) = shape(A)\n",
    "2. Initialize:\n",
    "       Q = zero matrix of size m×n\n",
    "       R = zero matrix of size n×n\n",
    "\n",
    "3. For k = 0, 1, ..., n−1:          # process column k of A\n",
    "\n",
    "       a_k = column k of A\n",
    "       v   = a_k                     # working copy\n",
    "\n",
    "       # Remove components along previously computed q_j\n",
    "       For j = 0, 1, ..., k−1:\n",
    "\n",
    "           r_{j,k} = (q_j)^T a_k     # inner product / projection coefficient\n",
    "           R[j, k] = r_{j,k}\n",
    "\n",
    "           v = v − r_{j,k} q_j       # subtract projection onto q_j\n",
    "\n",
    "       # Now v is orthogonal to all q_0, ..., q_{k−1}\n",
    "\n",
    "       r_{k,k} = ||v||_2             # Euclidean norm of v\n",
    "       R[k, k] = r_{k,k}\n",
    "\n",
    "       q_k = v / r_{k,k}             # normalize to unit length\n",
    "       set column k of Q to q_k\n",
    "\n",
    "4. Return Q, R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bad42b6-9146-4818-b455-2f20e2da0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_gram_schmidt(A):\n",
    "    \"\"\"\n",
    "    QR decomposition of A using (classical-ish) Gram-Schmidt.\n",
    "    A: m x n matrix (numpy array)\n",
    "    Returns Q (m x n), R (n x n) such that A = Q @ R\n",
    "    \"\"\"\n",
    "    A = np.array(A, dtype=float)\n",
    "    m, n = A.shape\n",
    "\n",
    "    Q = np.zeros((m, n))\n",
    "    R = np.zeros((n, n))\n",
    "\n",
    "    for k in range(n):\n",
    "        v = A[:, k].copy()\n",
    "        for j in range(k):\n",
    "            R[j, k] = np.dot(Q[:, j], A[:, k])\n",
    "            v = v - R[j, k] * Q[:, j]\n",
    "        R[k, k] = np.linalg.norm(v)\n",
    "        Q[:, k] = v / R[k, k]\n",
    "\n",
    "    return Q, R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f951b6be-f5b9-453c-92b4-d3d89cc57ce7",
   "metadata": {},
   "source": [
    "# Backward Substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf426bc-8e44-43d3-8e8f-f6260bc39c9d",
   "metadata": {},
   "source": [
    "Algorithm: Backward substitution for upper-triangular systems\n",
    "\n",
    "Goal:\n",
    "    Solve U x = b for x,\n",
    "    where U ∈ R^{n×n} is upper triangular\n",
    "    (i.e. U[i,j] = 0 for all i > j).\n",
    "\n",
    "Input:\n",
    "    U ∈ R^{n×n}  (upper-triangular, nonzero diagonal)\n",
    "    b ∈ R^{n}    (right-hand side vector)\n",
    "\n",
    "Output:\n",
    "    x ∈ R^{n}    (solution to U x = b)\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Let n = number of rows of U\n",
    "\n",
    "2. Initialize x as a length-n zero vector\n",
    "\n",
    "3. For i = n−1, n−2, ..., 0:       # loop backwards over rows\n",
    "\n",
    "       # Compute the sum of known terms U[i,j] * x[j]\n",
    "       s = 0\n",
    "       For j = i+1, i+2, ..., n−1:\n",
    "           s = s + U[i, j] * x[j]\n",
    "\n",
    "       # Solve for x[i] using the i-th equation:\n",
    "       #   U[i,i] * x[i] + sum_{j=i+1}^{n-1} U[i,j] x[j] = b[i]\n",
    "       # ⇒ x[i] = (b[i] − s) / U[i,i]\n",
    "       x[i] = (b[i] − s) / U[i, i]\n",
    "\n",
    "4. Return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57c6454d-b8fb-42c1-b688-76b3231b48e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_substitution(U, b):\n",
    "    \"\"\"\n",
    "    Solve U x = b for x, where U is an upper-triangular (n x n) matrix.\n",
    "    Uses plain backward substitution.\n",
    "    \"\"\"\n",
    "    U = np.asarray(U, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "\n",
    "    n, m = U.shape\n",
    "    if n != m:\n",
    "        raise ValueError(\"U must be square\")\n",
    "    if b.shape[0] != n:\n",
    "        raise ValueError(\"Dimension mismatch between U and b\")\n",
    "\n",
    "    x = np.zeros_like(b, dtype=float)\n",
    "\n",
    "    # Go from last row up to first\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        if U[i, i] == 0:\n",
    "            raise ValueError(\"Matrix is singular (zero on diagonal)\")\n",
    "\n",
    "        # sum_{j=i+1}^n u_{ij} x_j\n",
    "        s = 0.0\n",
    "        for j in range(i + 1, n):\n",
    "            s += U[i, j] * x[j]\n",
    "\n",
    "        x[i] = (b[i] - s) / U[i, i]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e6035-8983-461a-b1ec-9de1a12266f6",
   "metadata": {},
   "source": [
    "# OLS via Gram-Schmidt and Backward Substitution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075a7181-063b-40fc-aa0c-d57cd5d21899",
   "metadata": {},
   "source": [
    "$$RSS(w)= \\frac{1}{2} \\sum_{n=1}^{N}(y_{n}-w^{\\top}x_{n})^2=\\frac{1}{2}||Xw-y||_{2}^{2}=\\frac{1}{2}(Xw-y)^{\\top}(Xw-y)=\\frac{1}{2}X^{\\top}Xw^2-X^{\\top}yw+\\frac{1}{2}y^{\\top}y$$\n",
    "\n",
    "$$\\nabla RSS(w)=X^{\\top}Xw-X^{\\top}y \\overset{!}=0$$\n",
    "\n",
    "$$X^{\\top}Xw=X^{\\top}y \\Rightarrow Xw = y$$\n",
    "\n",
    "$$(QR)w=y\\Rightarrow Rw =Q^{\\top} y$$\n",
    "\n",
    "$$\\text{QR-decomposition}+\\text{Backward-Substitution}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94877cc5-b45c-4f9b-8ec3-93a6fa271e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_via_qr(X, y):\n",
    "    \"\"\"\n",
    "    Compute the OLS estimator beta_hat for y = X beta + eps\n",
    "    using QR decomposition (Gram-Schmidt) and back-substitution.\n",
    "\n",
    "    X: (n x p) design matrix\n",
    "    y: (n,) or (n,1) response vector\n",
    "\n",
    "    Returns:\n",
    "        beta_hat: (p,) vector of OLS coefficients\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float).reshape(-1)  \n",
    "\n",
    "    \n",
    "    Q, R = qr_gram_schmidt(X) \n",
    "    \n",
    "    b = Q.T @ y             \n",
    "\n",
    "    beta_hat = back_substitution(R, b)\n",
    "\n",
    "    return beta_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1584442-e13a-46b0-a8a1-785c7ae75a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True beta:      [ 1.5 -2.   0.5]\n",
      "Estimated beta: [ 1.46840505 -2.03466378  0.44907805]\n",
      "np.linalg.lstsq beta: [ 1.46840505 -2.03466378  0.44907805]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.44825316, -2.01574798,  0.44603553])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)   \n",
    "n = 100 \n",
    "p = 3 \n",
    "X = np.random.randn(n, p)\n",
    "beta_true = np.array([1.5, -2.0, 0.5])\n",
    "sigma = 0.5\n",
    "eps = sigma * np.random.randn(n)\n",
    "y = X @ beta_true + eps\n",
    "beta_hat = ols_via_qr(X, y)\n",
    "print(\"True beta:     \", beta_true)\n",
    "print(\"Estimated beta:\", beta_hat)\n",
    "beta_lstsq, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
    "print(\"np.linalg.lstsq beta:\", beta_lstsq)\n",
    "\n",
    "X = np.asarray(X, float)\n",
    "y = np.asarray(y, float)\n",
    "\n",
    "n, p = X.shape\n",
    "XtX = X.T @ X\n",
    "Xty = X.T @ y\n",
    "I = np.eye(p)\n",
    "lam = 1\n",
    "beta_ridge = np.linalg.solve(XtX + lam * I, Xty)\n",
    "beta_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "78ceee7a-a05d-4ddb-91ff-c17633cd9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_closed_form(X, y, lam):\n",
    "    \"\"\"\n",
    "    Closed-form ridge regression:\n",
    "        beta = (X^T X + lam * I)^(-1) X^T y\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : (n, p) array\n",
    "    y : (n,) array\n",
    "    lam : float, ridge penalty λ >= 0\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, float)\n",
    "    y = np.asarray(y, float).ravel()\n",
    "\n",
    "    n, p = X.shape\n",
    "    XtX = X.T @ X\n",
    "    Xty = X.T @ y\n",
    "    I = np.eye(p)\n",
    "    beta_ridge = np.linalg.solve(XtX + lam * I, Xty)\n",
    "    return beta_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c6c0815a-8562-43c1-af60-f4271e5a02af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.51209096, -1.92031595, -0.00426938,  0.56217399, -0.00372307])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n, p = 100, 5\n",
    "X = np.random.randn(n, p)\n",
    "beta_true = np.array([1.5, -2.0, 0.0, 0.5, 0.0])\n",
    "y = X @ beta_true + 0.5 * np.random.randn(n)\n",
    "\n",
    "lam = 1# L1 penalty\n",
    "beta_hat = ridge_regression_closed_form(X, y, lam)\n",
    "beta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5cd713-add3-464d-afde-25c89179ffd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd4e205-04b0-4544-9360-c88752040dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df9037-6bbc-4c49-bb78-9c78940212a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "630c44af-e0ae-4724-a60d-24da9e774aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True intercept ≈ 0.0 (because we simulated with no intercept)\n",
      "Estimated intercept: -0.11765437766603941\n",
      "True beta:      [ 1.5 -2.   0.   0.5  0. ]\n",
      "Estimated beta: [ 0.37727042 -1.04109465  0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def soft_threshold(x, lam):\n",
    "    if x > lam:\n",
    "        return x - lam\n",
    "    elif x < -lam:\n",
    "        return x + lam\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def lasso_coordinate_descent(X, y, lam, max_iter=1000, tol=1e-6):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "    n, p = X.shape\n",
    "\n",
    "    X_mean = X.mean(axis=0)\n",
    "    y_mean = y.mean()\n",
    "    Xc = X - X_mean\n",
    "    yc = y - y_mean\n",
    "\n",
    "    beta = np.zeros(p)\n",
    "    resid = yc.copy()\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        max_change = 0.0\n",
    "\n",
    "        for j in range(p):\n",
    "            xj = Xc[:, j]\n",
    "            resid += xj * beta[j]\n",
    "            rho_j = (xj @ resid) / n\n",
    "            z_j = (xj ** 2).sum() / n\n",
    "            if z_j == 0:\n",
    "                new_beta_j = 0.0\n",
    "            else:\n",
    "                new_beta_j = soft_threshold(rho_j, lam) / z_j\n",
    "\n",
    "            resid -= xj * new_beta_j\n",
    "            max_change = max(max_change, abs(new_beta_j - beta[j]))\n",
    "\n",
    "            beta[j] = new_beta_j\n",
    "\n",
    "        if max_change < tol:\n",
    "            break\n",
    "    beta0 = y_mean - X_mean @ beta\n",
    "    return beta0, beta\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    n, p = 100, 5\n",
    "    X = np.random.randn(n, p)\n",
    "    beta_true = np.array([1.5, -2.0, 0.0, 0.5, 0.0])\n",
    "    y = X @ beta_true + 0.5 * np.random.randn(n)\n",
    "\n",
    "    lam = 1# L1 penalty\n",
    "    beta0_hat, beta_hat = lasso_coordinate_descent(X, y, lam)\n",
    "\n",
    "    print(\"True intercept ≈ 0.0 (because we simulated with no intercept)\")\n",
    "    print(\"Estimated intercept:\", beta0_hat)\n",
    "    print(\"True beta:     \", beta_true)\n",
    "    print(\"Estimated beta:\", beta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c63f9-311f-4873-b4cb-59db3f7df32f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
